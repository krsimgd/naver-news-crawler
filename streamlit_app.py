import streamlit as st
from bs4 import BeautifulSoup
from datetime import datetime
import requests
import pandas as pd
import re

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬",
    page_icon="ğŸ“°",
    layout="wide"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5em;
        color: #1e3d59;
        text-align: center;
        margin-bottom: 1em;
        padding: 1em;
        border-radius: 10px;
        background: #f5f7fa;
    }
    .stButton>button {
        background-color: #1e3d59;
        color: white;
        padding: 0.5em 2em;
        border-radius: 5px;
        border: none;
        width: 100%;
    }
    .search-section {
        background-color: #f5f7fa;
        padding: 2em;
        border-radius: 10px;
        margin-bottom: 2em;
    }
    .results-section {
        background-color: #f5f7fa;
        padding: 2em;
        border-radius: 10px;
    }
    </style>
""", unsafe_allow_html=True)

# í—¤ë”
st.markdown("<h1 class='main-header'>ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬</h1>", unsafe_allow_html=True)

# ê²°ê³¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì„ ì–¸
title_text=[]
link_text=[]
source_text=[]
date_text=[]
contents_text=[]
result={}

# ë‚ ì§œ ì •ì œí™” í•¨ìˆ˜
def date_cleansing(test):
    try:
        pattern = '\d+.(\d+).(\d+).'
        r = re.compile(pattern)
        match = r.search(test).group(0)
        date_text.append(match)
    except AttributeError:
        pattern = '\w* (\d\w*)'
        r = re.compile(pattern)
        match = r.search(test).group(1)
        date_text.append(match)

# ë‚´ìš© ì •ì œí™” í•¨ìˆ˜
def contents_cleansing(contents):
    first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '', str(contents)).strip()
    second_cleansing_contents = re.sub('<ul class="relation_lst">.*?</dd>', '', first_cleansing_contents).strip()
    third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()
    contents_text.append(third_cleansing_contents)

# í¬ë¡¤ë§ í•¨ìˆ˜
def crawler(maxpage,query,sort,s_date,e_date):
    # ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    title_text.clear()
    link_text.clear()
    source_text.clear()
    date_text.clear()
    contents_text.clear()
    
    s_from = s_date.replace(".","")
    e_to = e_date.replace(".","")
    page = 1  
    maxpage_t = (int(maxpage)-1)*10+1
    
    while page <= maxpage_t:
        url = "https://search.naver.com/search.naver?where=news&query=" + query + "&sort="+sort+"&ds=" + s_date + "&de=" + e_date + "&nso=so%3Ar%2Cp%3Afrom" + s_from + "to" + e_to + "%2Ca%3A&start=" + str(page)
        
        response = requests.get(url)
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')
 
        atags = soup.select('.news_tit')
        for atag in atags:
            title_text.append(atag.text)
            link_text.append(atag['href'])
            
        source_lists = soup.select('.info_group > .press')
        for source_list in source_lists:
            source_text.append(source_list.text)
        
        date_lists = soup.select('.info_group > span.info')
        for date_list in date_lists:
            if date_list.text.find("ë©´") == -1:
                date_text.append(date_list.text)
        
        contents_lists = soup.select('.news_dsc')
        for contents_list in contents_lists:
            contents_cleansing(contents_list)
        
        page += 10
        
    result = {"ë‚ ì§œ" : date_text, "ì œëª©":title_text, "ì–¸ë¡ ì‚¬" : source_text, "ë‚´ìš©ìš”ì•½": contents_text, "ë§í¬":link_text}
    df = pd.DataFrame(result)
    return df

# ê²€ìƒ‰ ì„¹ì…˜
st.markdown("<div class='search-section'>", unsafe_allow_html=True)

col1, col2 = st.columns(2)

with col1:
    st.markdown("### ğŸ” ê²€ìƒ‰ ì„¤ì •")
    maxpage = st.number_input("ğŸ“‘ í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜", 
                            min_value=1, 
                            max_value=50, 
                            value=1,
                            help="í•œ í˜ì´ì§€ë‹¹ 10ê°œì˜ ë‰´ìŠ¤ê°€ ìˆ˜ì§‘ë©ë‹ˆë‹¤")
    
    query = st.text_input("ğŸ”¤ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                         placeholder="ì˜ˆ: ì¸ê³µì§€ëŠ¥")

with col2:
    st.markdown("### âš™ï¸ ìƒì„¸ ì„¤ì •")
    sort = st.selectbox("ì •ë ¬ ë°©ì‹", 
                       options=['0', '1', '2'],
                       format_func=lambda x: {
                           '0': 'ğŸ’¡ ê´€ë ¨ë„ìˆœ',
                           '1': 'â° ìµœì‹ ìˆœ',
                           '2': 'ğŸ“… ì˜¤ë˜ëœìˆœ'
                       }[x])
    
    s_date = st.text_input("ğŸ“… ì‹œì‘ë‚ ì§œ (ì˜ˆ: 2024.01.04)")
    e_date = st.text_input("ğŸ“… ì¢…ë£Œë‚ ì§œ (ì˜ˆ: 2024.01.05)")

st.markdown("</div>", unsafe_allow_html=True)

# ê²€ìƒ‰ ë²„íŠ¼
if st.button("ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘"):
    if query and s_date and e_date:
        with st.spinner('ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤... ğŸ”„'):
            df = crawler(maxpage,query,sort,s_date,e_date)
            
            # ê²°ê³¼ ì„¹ì…˜
            st.markdown("<div class='results-section'>", unsafe_allow_html=True)
            st.markdown("### ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼")
            st.success(f'âœ¨ {len(df)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!')
            
            # ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
            st.dataframe(df, height=400)
            
            # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            csv = df.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                "ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (CSV)",
                csv,
                f"naver_news_{query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                "text/csv"
            )
            st.markdown("</div>", unsafe_allow_html=True)
    else:
        st.error('âš ï¸ ê²€ìƒ‰ì–´ì™€ ë‚ ì§œë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”!')

# í˜ì´ì§€ í‘¸í„°
st.markdown("---")
st.markdown("""
    <div style='text-align: center; color: #666; padding: 1em;'>
    Made with â¤ï¸ using Streamlit<br>
    Last Updated: 2024.01
    </div>
""", unsafe_allow_html=True)
